
@inproceedings{galles_testing_1995,
	address = {San Francisco, CA, USA},
	series = {{UAI}'95},
	title = {Testing {Identifiability} of {Causal} {Effects}},
	isbn = {978-1-55860-385-1},
	url = {http://dl.acm.org/citation.cfm?id=2074158.2074180},
	abstract = {This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.},
	urldate = {2019-03-12},
	booktitle = {Proceedings of the {Eleventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Galles, David and Pearl, Judea},
	year = {1995},
	note = {event-place: Montréal, Qué, Canada},
	pages = {185--195},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/8ZXWUHLH/Galles and Pearl - 1995 - Testing Identifiability of Causal Effects.pdf:application/pdf}
}


@inproceedings{bareinboim_local_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Local {Characterizations} of {Causal} {Bayesian} {Networks}},
	isbn = {978-3-642-29449-5},
	abstract = {The standard definition of causal Bayesian networks (CBNs) invokes a global condition according to which the distribution resulting from any intervention can be decomposed into a truncated product dictated by its respective mutilated subgraph. We analyze alternative formulations which emphasizes local aspects of the causal process and can serve therefore as more meaningful criteria for coherence testing and network construction. We first examine a definition based on “modularity” and prove its equivalence to the global definition. We then introduce two new definitions, the first interprets the missing edges in the graph, and the second interprets “zero direct effect” (i.e., ceteris paribus). We show that these formulations are equivalent but carry different semantic content.},
	language = {en},
	booktitle = {Graph {Structures} for {Knowledge} {Representation} and {Reasoning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bareinboim, Elias and Brito, Carlos and Pearl, Judea},
	editor = {Croitoru, Madalina and Rudolph, Sebastian and Wilson, Nic and Howse, John and Corby, Olivier},
	year = {2012},
	keywords = {Causal Information, Directed Acyclic Graph, Interventional Distribution, Manipulate Variable, Probabilistic Interpretation},
	pages = {1--17},
	file = {Springer Full Text PDF:/home/users/u4533535/Zotero/storage/6F7JA6K9/Bareinboim et al. - 2012 - Local Characterizations of Causal Bayesian Network.pdf:application/pdf}
}

@book{angrist_mastering_2014,
	address = {Princeton ; Oxford},
	edition = {with French flaps edition},
	title = {Mastering '{Metrics}: {The} {Path} from {Cause} to {Effect}},
	isbn = {978-0-691-15284-4},
	shorttitle = {Mastering '{Metrics}},
	abstract = {Applied econometrics, known to aficionados as 'metrics, is the original data science. 'Metrics encompasses the statistical methods economists use to untangle cause and effect in human affairs. Through accessible discussion and with a dose of kung fu–themed humor, Mastering 'Metrics presents the essential tools of econometric research and demonstrates why econometrics is exciting and useful.The five most valuable econometric methods, or what the authors call the Furious Five--random assignment, regression, instrumental variables, regression discontinuity designs, and differences in differences--are illustrated through well-crafted real-world examples (vetted for awesomeness by Kung Fu Panda's Jade Palace). Does health insurance make you healthier? Randomized experiments provide answers. Are expensive private colleges and selective public high schools better than more pedestrian institutions? Regression analysis and a regression discontinuity design reveal the surprising truth. When private banks teeter, and depositors take their money and run, should central banks step in to save them? Differences-in-differences analysis of a Depression-era banking crisis offers a response. Could arresting O. J. Simpson have saved his ex-wife's life? Instrumental variables methods instruct law enforcement authorities in how best to respond to domestic abuse.Wielding econometric tools with skill and confidence, Mastering 'Metrics uses data and statistics to illuminate the path from cause to effect.Shows why econometrics is importantExplains econometric research through humorous and accessible discussionOutlines empirical methods central to modern econometric practiceWorks through interesting and relevant real-world examples},
	language = {English},
	publisher = {Princeton University Press},
	author = {Angrist, Joshua D. and Pischke, Jörn-Steffen},
	month = dec,
	year = {2014}
}

@article{evans_graphs_2016,
	title = {Graphs for {Margins} of {Bayesian} {Networks}},
	volume = {43},
	copyright = {© 2015 Board of the Foundation of the Scandinavian Journal of Statistics},
	issn = {1467-9469},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12194},
	doi = {10.1111/sjos.12194},
	abstract = {Directed acyclic graph (DAG) models—also called Bayesian networks—are widely used in probabilistic reasoning, machine learning and causal inference. If latent variables are present, then the set of possible marginal distributions over the remaining (observed) variables is generally not represented by any DAG. Larger classes of mixed graphical models have been introduced to overcome this; however, as we show, these classes are not sufficiently rich to capture all the marginal models that can arise. We introduce a new class of hyper-graphs, called mDAGs, and a latent projection operation to obtain an mDAG from the margin of a DAG. We show that each distinct marginal of a DAG model is represented by at least one mDAG and provide graphical results towards characterizing equivalence of these models. Finally, we show that mDAGs correctly capture the marginal structure of causally interpreted DAGs under interventions on the observed variables.},
	language = {en},
	number = {3},
	urldate = {2019-02-11},
	journal = {Scandinavian Journal of Statistics},
	author = {Evans, Robin J.},
	year = {2016},
	keywords = {causal model, directed acyclic graph, latent variable},
	pages = {625--648},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\E7DBNDH5\\Evans - 2016 - Graphs for Margins of Bayesian Networks.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\MSZY4I4N\\sjos.html:text/html}
}


@book{devroye_non-uniform_1986,
	address = {New York},
	title = {Non-{Uniform} {Random} {Variate} {Generation}},
	isbn = {978-1-4613-8645-2},
	url = {//www.springer.com/gp/book/9781461386452},
	abstract = {Non-Uniform Random Variate Generation...},
	language = {en},
	urldate = {2019-01-29},
	publisher = {Springer-Verlag},
	author = {Devroye, Luc},
	year = {1986}
}

@article{buja2005loss,
  title={Loss functions for binary class probability estimation and classification: Structure and applications},
  author={Buja, Andreas and Stuetzle, Werner and Shen, Yi},
  year={2005}
}


@article{wald1950statistical,
  title={Statistical decision functions.},
  author={Wald, Abraham},
  year={1950},
  publisher={Wiley}
}


@techreport{heckman_randomization_1991,
	type = {Working {Paper}},
	title = {Randomization and {Social} {Policy} {Evaluation}},
	url = {http://www.nber.org/papers/t0107},
	abstract = {This paper considers the recent case for randomized social experimentation and contrasts it with older cases for social experimentation. The recent case eschews behavioral models, assumes that certain mean differences in outcomes are the parameters of interest to evaluators and assumes that randomization does not disrupt the social program being analyzed. Conditions under which program disruption effects are of no consequence are presented. Even in the absence of randomization bias, ideal experimental data cannot estimate median (other quantile) differences between treated and untreated persons without invoking supplementary statistical assumptions. The recent case for randomized experimentation does not address the choice of the appropriate stage in a multistage program at which randomization should be conducted. Evidence on randomization bias is presented.},
	number = {107},
	urldate = {2019-01-15},
	institution = {National Bureau of Economic Research},
	author = {Heckman, James J},
	month = jul,
	year = {1991},
	doi = {10.3386/t0107},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\B4R6Y5I2\\Heckman - 1991 - Randomization and Social Policy Evaluation.pdf:application/pdf}
}

@phdthesis{shpitser_complete_2008,
	address = {Los Angeles, CA, USA},
	type = {{PhD} {Thesis}},
	title = {Complete {Identification} {Methods} for {Causal} {Inference}},
	abstract = {Human beings organize their intuitive understanding of the world in terms of causes and effects. Primitive humanity posited gods and spirits as invisible causes of phenomena they did not comprehend. As our attempts to understand the world began to be formalized and codified as empirical science, the emphasis on discerning cause-effect relationships remained. Though we, the modern humanity, are armed with powerful computers, sophisticated technology, and highly developed mathematics and statistics, our fundamental questions remain the same as those of our cave dwelling ancestors – we seek to understand the causes of windfalls and misfortunes that befall us, what effects our actions have, and what would happen if the past were different from what it is. This thesis will address these ancient questions with the rigor and generality of modern mathematics. Using the framework of graphical causal models which formalizes a variety of causal queries, such as causal effects, counterfactuals and path-specific effects as certain types of probability distributions, I will develop algorithms which will evaluate these probability distributions from available information; prove that whenever these algorithms fail to evaluate a query, no other method could succeed; provide characterizations based on directed graphs for cases where these algorithms do succeed; and finally show how a class of constraints placed on the causal model by its directed graph are due to conditional independence in these probability distributions, and how these conditional independencies can be exploited for testing causal theories.},
	school = {University of California at Los Angeles},
	author = {Shpitser, Ilya},
	year = {2008},
	annote = {AAI3351728}
}

@article{egan_counterexamples_2007,
	title = {Some {Counterexamples} to {Causal} {Decision} {Theory}},
	volume = {116},
	issn = {0031-8108},
	url = {https://www.jstor.org/stable/20446939},
	number = {1},
	urldate = {2019-01-11},
	journal = {The Philosophical Review},
	author = {Egan, Andy},
	year = {2007},
	pages = {93--114}
}

@article{lewis_causal_1981,
	title = {Causal decision theory},
	volume = {59},
	issn = {0004-8402},
	url = {https://doi.org/10.1080/00048408112340011},
	doi = {10.1080/00048408112340011},
	abstract = {Newcomb's problem and similar cases show the need to incorporate causal distinctions into the theory of rational decision; the usual noncausal decision theory, though simpler, does not always give the right answers. I give my own version of causal decision theory, compare it with versions offered by several other authors, and suggest that the versions have more in common than meets the eye.},
	number = {1},
	urldate = {2019-01-11},
	journal = {Australasian Journal of Philosophy},
	author = {Lewis, David},
	month = mar,
	year = {1981},
	pages = {5--30}
}

@misc{horgan_counterfactuals_1981,
	title = {Counterfactuals and {Newcomb}'s {Problem}},
	url = {https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform&fp=jphil&id=jphil_1981_0078_0006_0331_0356},
	urldate = {2019-01-08},
	journal = {The Journal of Philosophy},
	author = {Horgan, Terence},
	month = jun,
	year = {1981},
	doi = {10.2307/2026128},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\YNXJ7XHN\\jphil_1981_0078_0006_0331_0356.html:text/html}
}

@misc{noauthor_preliminary_nodate,
	title = {Preliminary {Survey} results {\textbar} {PhilPapers} {Surveys}},
	url = {https://philpapers.org/surveys/results.pl?affil=All+respondents&areas0=0&areas_max=1&grain=fine},
	language = {en},
	urldate = {2018-12-20}
}


@misc{noauthor_resolving_2009,
	title = {Resolving disputes between {J}. {Pearl} and {D}. {Rubin} on causal inference},
	url = {https://andrewgelman.com/2009/07/05/disputes_about/},
	abstract = {This is a pretty long one. It’s an attempt to explore some of the differences between Judea Pearl’s and Don Rubin’s approaches to causal inference, and is motivated by recent article by Pearl. Pearl sent me a link to this piece of his, writing: I [Pearl] would like to encourage a blog-discussion on the main …},
	language = {en-US},
	urldate = {2018-12-20},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	month = jul,
	year = {2009},
	author={Gelman, Andrew},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\WUHG43MZ\\disputes_about.html:text/html}
}


@article{sjolander_propensity_2009,
	title = {Propensity scores and {M}-structures},
	volume = {28},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3532},
	doi = {10.1002/sim.3532},
	abstract = {In a recent issue of Statistics in Medicine, Ian Shrier [Statist. Med. 2008; 27(14):2740–2741] posed a question regarding the use of propensity scores [Biometrika 1983; 70(1):41–55]. He considered an ‘M-structure’ illustrated by the directed acyclic graph (DAG) in Figure 1. In Figure 1, z is a binary exposure, r is a response of interest, x is a measured covariate, and u1 and u2 are two unmeasured covariates. Shrier stated that for the M-structure, ‘... it remains unclear if the propensity method described by Rubin would introduce selection bias or not’. In the same issue, Donald Rubin [Statist. Med. 2002; 27(14):2741–2742] replied by clarifying several key points in the use of propensity scores. He did not, however, discuss the original question posed by Shrier. Given the popularity of both propensity score methods and graphical models, I think any confusion regarding the appropriateness of these methods deserves serious attention and I would therefore like to answer Shrier's question here. The short answer is that for the M-structure, propensity score methods do indeed induce a bias. Below, I will clarify this statement. I will first briefly review the basic idea of propensity scores and then explain why the idea does not apply to the M-structure. I will use a notation which is consistent with Rosenbaum and Rubin [Biometrika 1983; 70(1):41–55]. Copyright © 2009 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {9},
	urldate = {2018-12-19},
	journal = {Statistics in Medicine},
	author = {Sjölander, Arvid},
	year = {2009},
	keywords = {DAG, M-structure, propensity score},
	pages = {1416--1420},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\LCWWNTBS\\sim.html:text/html}
}


@article{rubin_authors_2008,
	title = {Author's {Reply}},
	volume = {27},
	copyright = {Copyright © 2008 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3231},
	doi = {10.1002/sim.3231},
	language = {en},
	number = {14},
	urldate = {2018-12-19},
	journal = {Statistics in Medicine},
	author = {Rubin, Donald B.},
	year = {2008},
	pages = {2741--2742},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\8C6BTIL9\\Rubin - 2008 - Author's Reply.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\77QLCQXK\\sim.html:text/html}
}

@article{heckman_policy-relevant_2001,
	title = {Policy-{Relevant} {Treatment} {Effects}},
	volume = {91},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/2677742},
	number = {2},
	urldate = {2018-12-19},
	journal = {The American Economic Review},
	author = {Heckman, James J. and Vytlacil, Edward},
	year = {2001},
	pages = {107--111}
}

@article{shalit_estimating_2016,
	title = {Estimating individual treatment effect: generalization bounds and algorithms},
	shorttitle = {Estimating individual treatment effect},
	url = {http://arxiv.org/abs/1606.03976},
	abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
	urldate = {2018-12-18},
	journal = {arXiv:1606.03976 [cs, stat]},
	author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03976},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Added name "TARNet" to refer to version with alpha = 0. Removed supp}
}

@techreport{heckman_structural_2005,
	type = {Working {Paper}},
	title = {Structural {Equations}, {Treatment} {Effects} and {Econometric} {Policy} {Evaluation}},
	url = {http://www.nber.org/papers/w11259},
	abstract = {This paper uses the marginal treatment effect (MTE) to unify the nonparametric literature on treatment effects with the econometric literature on structural estimation using a nonparametric analog of a policy invariant parameter; to generate a variety of treatment effects from a common semiparametric functional form; to organize the literature on alternative estimators; and to explore what policy questions commonly used estimators in the treatment effect literature answer. A fundamental asymmetry intrinsic to the method of instrumental variables is noted. Recent advances in IV estimation allow for heterogeneity in responses but not in choices, and the method breaks down when both choice and response equations are heterogeneous in a general way.},
	number = {11259},
	urldate = {2018-02-06},
	institution = {National Bureau of Economic Research},
	author = {Heckman, James J. and Vytlacil, Edward},
	month = apr,
	year = {2005},
	doi = {10.3386/w11259},
	file = {NBER Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\JJDUJQ6S\\Heckman and Vytlacil - 2005 - Structural Equations, Treatment Effects and Econom.pdf:application/pdf}
}

@book{peters_elements_2017,
	title = {Elements of {Causal} {Inference}},
	publisher = {MIT Press},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernard},
	year = {2017},
	file = {Elements of Causal Inference Foundations and Learning Algorithms.pdf:/home/users/u4533535/Zotero/storage/PINE9XEV/Elements of Causal Inference Foundations and Learning Algorithms.pdf:application/pdf}
}

@article{cartwright_causation:_2004,
	title = {Causation: {One} {Word}, {Many} {Things}},
	volume = {71},
	issn = {0031-8248},
	shorttitle = {Causation},
	url = {https://www.jstor.org/stable/10.1086/426771},
	doi = {10.1086/426771},
	abstract = {We currently have on offer a variety of different theories of causation. Many are strikingly good, providing detailed and plausible treatments of exemplary cases; and all suffer from clear counterexamples. I argue that, contra Hume and Kant, this is because causation is not a single, monolithic concept. There are different kinds of causal relations imbedded in different kinds of systems, readily described using thick causal concepts. Our causal theories pick out important and useful structures that fit some familiar cases—cases we discover and ones we devise to fit.},
	number = {5},
	urldate = {2018-12-17},
	journal = {Philosophy of Science},
	author = {Cartwright, Nancy},
	year = {2004},
	pages = {805--819},
	file = {JSTOR Full Text PDF:/home/users/u4533535/Zotero/storage/NCSVX2W3/Cartwright - 2004 - Causation One Word, Many Things.pdf:application/pdf}
}

@misc{hartnett_build_nodate,
	title = {To {Build} {Truly} {Intelligent} {Machines}, {Teach} {Them} {Cause} and {Effect}},
	url = {https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/},
	abstract = {Judea Pearl, a pioneering figure in artificial intelligence, argues that AI has been stuck in a decades-long rut. His prescription for progress? Teach machines},
	urldate = {2018-12-17},
	journal = {Quanta Magazine},
	author = {Hartnett, Kevin}
}

@article{lemeire_replacing_2013,
	title = {Replacing {Causal} {Faithfulness} with {Algorithmic} {Independence} of {Conditionals}},
	volume = {23},
	issn = {0924-6495, 1572-8641},
	url = {https://link.springer.com/article/10.1007/s11023-012-9283-1},
	doi = {10.1007/s11023-012-9283-1},
	abstract = {Independence of Conditionals (IC) has recently been proposed as a basic rule for causal structure learning. If a Bayesian network represents the causal structure, its Conditional Probability Distributions (CPDs) should be algorithmically independent. In this paper we compare IC with causal faithfulness (FF), stating that only those conditional independences that are implied by the causal Markov condition hold true. The latter is a basic postulate in common approaches to causal structure learning. The common spirit of FF and IC is to reject causal graphs for which the joint distribution looks ‘non-generic’. The difference lies in the notion of genericity: FF sometimes rejects models just because one of the CPDs is simple, for instance if the CPD describes a deterministic relation. IC does not behave in this undesirable way. It only rejects a model when there is a non-generic relation between different CPDs although each CPD looks generic when considered separately. Moreover, it detects relations between CPDs that cannot be captured by conditional independences. IC therefore helps in distinguishing causal graphs that induce the same conditional independences (i.e., they belong to the same Markov equivalence class). The usual justification for FF implicitly assumes a prior that is a probability density on the parameter space. IC can be justified by Solomonoff’s universal prior, assigning non-zero probability to those points in parameter space that have a finite description. In this way, it favours simple CPDs, and therefore respects Occam’s razor. Since Kolmogorov complexity is uncomputable, IC is not directly applicable in practice. We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms.},
	language = {en},
	number = {2},
	urldate = {2018-05-24},
	journal = {Minds and Machines},
	author = {Lemeire, Jan and Janzing, Dominik},
	month = may,
	year = {2013},
	pages = {227--249},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/VSRYINKU/Lemeire and Janzing - 2013 - Replacing Causal Faithfulness with Algorithmic Ind.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/DCRU8TNB/s11023-012-9283-1.html:text/html}
}


@book{pearl_causality:_2009,
	edition = {2},
	title = {Causality: {Models}, {Reasoning} and {Inference}},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009}
}



@article{chickering_optimal_2003,
	title = {Optimal {Structure} {Identification} with {Greedy} {Search}},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303321897717},
	doi = {10.1162/153244303321897717},
	abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	urldate = {2018-02-27},
	journal = {J. Mach. Learn. Res.},
	author = {Chickering, David Maxwell},
	month = mar,
	year = {2003},
	pages = {507--554},
	annote = {It's possible to identify the "true" data-generating DAG in the infinite sample limit using a two-phase local search algorithm. This follows from the fact that it's possible to},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/FIK5RCSU/Chickering - 2003 - Optimal Structure Identification with Greedy Searc.pdf:application/pdf}
}


@book{spirtes_causation_1993,
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	abstract = {What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences. The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables. The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection. The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	month = jan,
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/Q5VT529X/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:application/pdf}
}


@article{evans_margins_2015,
	title = {Margins of discrete {Bayesian} networks},
	url = {http://arxiv.org/abs/1501.02103},
	abstract = {Bayesian network models with latent variables are widely used in statistics and machine learning. In this paper we provide a complete algebraic characterization of Bayesian network models with latent variables when the observed variables are discrete and no assumption is made about the state-space of the latent variables. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. In particular these two models have the same dimension. The nested Markov model is therefore the best possible description of the latent variable model that avoids consideration of inequalities, which are extremely complicated in general. A consequence of this is that the constraint finding algorithm of Tian and Pearl (UAI 2002, pp519-527) is complete for finding equality constraints. Latent variable models suffer from difficulties of unidentifiable parameters and non-regular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization.},
	urldate = {2018-02-20},
	journal = {arXiv:1501.02103 [math, stat]},
	author = {Evans, Robin J.},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02103},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: 41 pages},
	annote = {Shows that nested markov models have the same dimension as marginal DAGs, which model DAGs with hidden variables. Nested markov models furthermore respect all equality constraints of mDAGs, though mDAGs might respect further inequality constraints; Tian and Pearl (related) show how to derive these equality constraints},
	file = {arXiv\:1501.02103 PDF:/home/users/u4533535/Zotero/storage/L9ALWTEB/Evans - 2015 - Margins of discrete Bayesian networks.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/ZAXBMMS3/1501.html:text/html}
}


@article{kang_inequality_2012,
	title = {Inequality {Constraints} in {Causal} {Models} with {Hidden} {Variables}},
	url = {http://arxiv.org/abs/1206.6829},
	abstract = {We present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network, in which some of the variables remain unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental distributions. The results have applications in testing causal models with observational or experimental data.},
	urldate = {2018-02-20},
	journal = {arXiv:1206.6829 [cs, stat]},
	author = {Kang, Changsung and Tian, Jin},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6829},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)},
	file = {arXiv\:1206.6829 PDF:/home/users/u4533535/Zotero/storage/TBWYF3KN/Kang and Tian - 2012 - Inequality Constraints in Causal Models with Hidde.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PNZ4JUK5/1206.html:text/html}
}


@article{yang_characterizing_2018,
	title = {Characterizing and {Learning} {Equivalence} {Classes} of {Causal} {DAGs} under {Interventions}},
	url = {http://arxiv.org/abs/1802.06310},
	abstract = {We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser and B{\textbackslash}"uhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.},
	urldate = {2018-06-06},
	journal = {arXiv:1802.06310 [math, stat]},
	author = {Yang, Karren D. and Katcoff, Abigail and Uhler, Caroline},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06310},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Methodology},
	annote = {Comment: 18 pages, 7 figures},
	file = {arXiv\:1802.06310 PDF:/home/users/u4533535/Zotero/storage/QYBLHGF6/Yang et al. - 2018 - Characterizing and Learning Equivalence Classes of.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/9XLAKWAE/1802.html:text/html}
}


@article{hauser_characterization_2012,
	title = {Characterization and {Greedy} {Learning} of {Interventional} {Markov} {Equivalence} {Classes} of {Directed} {Acyclic} {Graphs}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/hauser12a.html},
	number = {Aug},
	urldate = {2018-06-06},
	journal = {Journal of Machine Learning Research},
	author = {Hauser, Alain and Bühlmann, Peter},
	year = {2012},
	pages = {2409--2464},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/AR23G9FQ/Hauser and Bühlmann - 2012 - Characterization and Greedy Learning of Interventi.pdf:application/pdf;Snapshot:/home/users/u4533535/Zotero/storage/25U334TM/hauser12a.html:text/html}
}


@article{meek_strong_2013,
	title = {Strong {Completeness} and {Faithfulness} in {Bayesian} {Networks}},
	url = {https://arxiv.org/abs/1302.4973},
	language = {en},
	urldate = {2018-05-21},
	author = {Meek, Christopher},
	month = feb,
	year = {2013},
	file = {Full Text PDF:/home/users/u4533535/Zotero/storage/4JIRGSD9/Meek - 2013 - Strong Completeness and Faithfulness in Bayesian N.pdf:application/pdf}
}


@article{peters_causal_2014,
	title = {Causal {Discovery} with {Continuous} {Additive} {Noise} {Models}},
	volume = {15},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2670315},
	abstract = {Loading...},
	number = {1},
	urldate = {2018-05-22},
	journal = {J. Mach. Learn. Res.},
	author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Schölkopf, Bernhard},
	month = jan,
	year = {2014},
	keywords = {additive noise, Bayesian networks, causal inference, causal minimality, identifiability, structural equation models},
	pages = {2009--2053},
	file = {ACM Full Text PDF:/home/users/u4533535/Zotero/storage/6DNIF3QP/Peters et al. - 2014 - Causal Discovery with Continuous Additive Noise Mo.pdf:application/pdf}
}


@article{yudkowsky_functional_2017,
	title = {Functional {Decision} {Theory}: {A} {New} {Theory} of {Instrumental} {Rationality}},
	shorttitle = {Functional {Decision} {Theory}},
	url = {http://arxiv.org/abs/1710.05060},
	abstract = {This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, "Which output of this very function would yield the best outcome?" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.},
	urldate = {2018-07-02},
	journal = {arXiv:1710.05060 [cs]},
	author = {Yudkowsky, Eliezer and Soares, Nate},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05060},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1710.05060 PDF:/home/users/u4533535/Zotero/storage/62TI8YLX/Yudkowsky and Soares - 2017 - Functional Decision Theory A New Theory of Instru.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/PQ7QQZMF/1710.html:text/html}
}

@article{savage_theory_1951,
	title = {The theory of statistical decision},
	volume = {46},
	issn = {1537-274X(Electronic),0162-1459(Print)},
	doi = {10.2307/2280094},
	abstract = {Abraham Wald's recent book, Statistical Decision Functions, presents a new theory of the foundations of statistics. The vigorous exploration of this theory was begun by Professor Wald five or six years ago and is being continued under his leadership. The critical and philosophical remarks in this exposition may not accurately represent the views of Professor Wald, for both in writing and lecturing, he prefers to be rather non-committal on such points. Wald's report on the current state of the theory of statistical decision is of great scholarly value, and its possible influence for the good on statistics, through the enthusiastic few who are able to study it, is inestimable. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Journal of the American Statistical Association},
	author = {Savage, L. J.},
	year = {1951},
	pages = {55--67},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\CII4AC78\\1951-06617-001.html:text/html}
}

@article{galles_testing_2013,
	title = {Testing {Identifiability} of {Causal} {Effects}},
	url = {http://arxiv.org/abs/1302.4948},
	abstract = {This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.},
	urldate = {2018-07-09},
	journal = {arXiv:1302.4948 [cs]},
	author = {Galles, David and Pearl, Judea},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4948},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)},
	file = {arXiv\:1302.4948 PDF:C\:\\Users\\david\\Zotero\\storage\\R27PJQGJ\\Galles and Pearl - 2013 - Testing Identifiability of Causal Effects.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\7KEGAEMG\\1302.html:text/html}
}


@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}},
	volume = {100},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000001880},
	doi = {10.1198/016214504000001880},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	number = {469},
	urldate = {2018-07-09},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	month = mar,
	year = {2005},
	keywords = {Analysis of covariance, Assignment mechanism, Assignment-based causal inference, Bayesian inference, Direct causal effects, Fieller–Creasy, Fisher, Neyman, Observational studies, Principal stratification, Randomized experiments, Rubin causal model},
	pages = {322--331},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\UCYCX9EZ\\Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\FL3D7Q29\\016214504000001880.html:text/html}
}


@article{schurz_causality_2016,
	title = {Causality as a theoretical concept: explanatory warrant and empirical content of the theory of causal nets},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Causality as a theoretical concept},
	url = {https://link.springer.com/article/10.1007/s11229-014-0630-z},
	doi = {10.1007/s11229-014-0630-z},
	abstract = {We start this paper by arguing that causality should, in analogy with force in Newtonian physics, be understood as a theoretical concept that is not explicated by a single definition, but by the axioms of a theory. Such an understanding of causality implicitly underlies the well-known theory of causal (Bayes) nets (TCN) and has been explicitly promoted by Glymour (Br J Philos Sci 55:779–790, 2004). In this paper we investigate the explanatory warrant and empirical content of TCN. We sketch how the assumption of directed cause–effect relations can be philosophically justified by an inference to the best explanation. We then ask whether the explanations provided by TCN are merely post-facto or have independently testable empirical content. To answer this question we develop a fine-grained axiomatization of TCN, including a distinction of different kinds of faithfulness. A number of theorems show that although the core axioms of TCN are empirically empty, extended versions of TCN have successively increasing empirical content.},
	language = {en},
	number = {4},
	urldate = {2018-03-27},
	journal = {Synthese},
	author = {Schurz, Gerhard and Gebharter, Alexander},
	month = apr,
	year = {2016},
	pages = {1073--1103},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\Y6W8FAUJ\\Schurz and Gebharter - 2016 - Causality as a theoretical concept explanatory wa.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\ZXGFDGPV\\s11229-014-0630-z.html:text/html}
}


@article{ramsey_adjacency-faithfulness_2012,
	title = {Adjacency-{Faithfulness} and {Conservative} {Causal} {Inference}},
	url = {http://arxiv.org/abs/1206.6843},
	abstract = {Most causal inference algorithms in the literature (e.g., Pearl (2000), Spirtes et al. (2000), Heckerman et al. (1999)) exploit an assumption usually referred to as the causal Faithfulness or Stability condition. In this paper, we highlight two components of the condition used in constraint-based algorithms, which we call "Adjacency-Faithfulness" and "Orientation-Faithfulness". We point out that assuming Adjacency-Faithfulness is true, it is in principle possible to test the validity of Orientation-Faithfulness. Based on this observation, we explore the consequence of making only the Adjacency-Faithfulness assumption. We show that the familiar PC algorithm has to be modified to be (asymptotically) correct under the weaker, Adjacency-Faithfulness assumption. Roughly the modified algorithm, called Conservative PC (CPC), checks whether Orientation-Faithfulness holds in the orientation phase, and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However, if the stronger, standard causal Faithfulness condition actually obtains, the CPC algorithm is shown to output the same pattern as the PC algorithm does in the large sample limit. We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes. We end our paper by discussing how score-based algorithms such as GES perform when the Adjacency-Faithfulness but not the standard causal Faithfulness condition holds, and how to extend our work to the FCI algorithm, which allows for the possibility of latent variables.},
	urldate = {2018-02-26},
	journal = {arXiv:1206.6843 [cs, stat]},
	author = {Ramsey, Joseph and Zhang, Jiji and Spirtes, Peter L.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6843},
	keywords = {Statistics - Methodology, Computer Science - Artificial Intelligence},
	file = {arXiv\:1206.6843 PDF:C\:\\Users\\david\\Zotero\\storage\\5J3TGVYR\\Ramsey et al. - 2012 - Adjacency-Faithfulness and Conservative Causal Inf.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\david\\Zotero\\storage\\8VWWK4MQ\\1206.html:text/html}
}


@misc{sontag_causal_nodate,
	title = {Causal {Inference} for {Observational} {Studies}: {ICML} 2016 {Tutorial}},
	url = {https://cs.nyu.edu/~shalit/tutorial.html},
	urldate = {2018-01-26},
	author = {Sontag, David and Shalit, Uri},
	file = {Causal Inference for Observational Studies\: ICML 2016 Tutorial:/home/david/Zotero/storage/QMLPNVMY/tutorial.html:text/html}
}


@article{uhler_geometry_2013,
	title = {Geometry of the faithfulness assumption in causal inference},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.0547},
	doi = {10.1214/12-AOS1080},
	abstract = {Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.},
	number = {2},
	urldate = {2018-03-26},
	journal = {The Annals of Statistics},
	author = {Uhler, Caroline and Raskutti, Garvesh and Bühlmann, Peter and Yu, Bin},
	month = apr,
	year = {2013},
	note = {arXiv: 1207.0547},
	keywords = {Mathematics - Statistics Theory},
	pages = {436--463},
	annote = {For linear models with Gaussian noise, strong-faithfulness fails for {\textasciitilde}all models with {\textgreater}10 variables.
 
Comment: Published in at http://dx.doi.org/10.1214/12-AOS1080 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.0547 PDF:/home/users/u4533535/Zotero/storage/JSVPBL65/Uhler et al. - 2013 - Geometry of the faithfulness assumption in causal .pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/CA726KEA/1207.html:text/html}
}




@article{collaboration_cochrane_nodate,
	title = {Cochrane {Reviewers}' {Handbook} 4.2.1},
	author = {Collaboration, The Cochrane},
	pages = {241},
	file = {Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:/home/users/u4533535/Zotero/storage/P4V3SIQJ/Collaboration - Cochrane Reviewers' Handbook 4.2.1.pdf:application/pdf}
}


@book{cinlar_probability_2011,
	address = {New York},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Probability and {Stochastics}},
	isbn = {978-0-387-87858-4},
	url = {//www.springer.com/gp/book/9780387878584},
	abstract = {This text is an introduction to the modern theory and applications of probability and stochastics. The style and coverage is geared towards the theory of stochastic processes, but with some attention to the applications. In many instances the gist of the problem is introduced in practical, everyday language and then is made precise in mathematical form. The first four chapters are on probability theory: measure and integration, probability spaces, conditional expectations, and the classical limit theorems. There follows chapters on martingales, Poisson random measures, Levy Processes, Brownian motion, and Markov Processes.Special attention is paid to Poisson random measures and their roles in regulating the excursions of Brownian motion and the jumps of Levy and Markov processes. Each chapter has a large number of varied examples and exercises. The book is based on the author’s lecture notes in courses offered over the years at Princeton University. These courses attracted graduate students from engineering, economics, physics, computer sciences, and mathematics. Erhan Çinlar has received many awards for excellence in teaching, including the President’s Award for Distinguished Teaching at Princeton University. His research interests include theories of Markov processes, point processes, stochastic calculus, and stochastic flows. The book is full of insights and observations that only a lifetime researcher in probability can have, all told in a lucid yet precise style.},
	language = {en},
	urldate = {2018-08-07},
	publisher = {Springer-Verlag},
	author = {Çınlar, Erhan},
	year = {2011},
	file = {Snapshot:C\:\\Users\\david\\Zotero\\storage\\I8JJCEJC\\9780387878584.html:text/html}
}


@article{fong_causal_2013,
	title = {Causal {Theories}: {A} {Categorical} {Perspective} on {Bayesian} {Networks}},
	shorttitle = {Causal {Theories}},
	url = {http://arxiv.org/abs/1301.6201},
	abstract = {In this dissertation we develop a new formal graphical framework for causal reasoning. Starting with a review of monoidal categories and their associated graphical languages, we then revisit probability theory from a categorical perspective and introduce Bayesian networks, an existing structure for describing causal relationships. Motivated by these, we propose a new algebraic structure, which we term a causal theory. These take the form of a symmetric monoidal category, with the objects representing variables and morphisms ways of deducing information about one variable from another. A major advantage of reasoning with these structures is that the resulting graphical representations of morphisms match well with intuitions for flows of information between these variables. These categories can then be modelled in other categories, providing concrete interpretations for the variables and morphisms. In particular, we shall see that models in the category of measurable spaces and stochastic maps provide a slight generalisation of Bayesian networks, and naturally form a category themselves. We conclude with a discussion of this category, classifying the morphisms and discussing some basic universal constructions. ERRATA: (i) Pages 41-42: Objects of a causal theory are words, not collections, in \$V\$, and we include swaps as generating morphisms, subject to the identities defining a symmetric monoidal category. (ii) Page 46: A causal model is a strong symmetric monoidal functor.},
	urldate = {2018-08-08},
	journal = {arXiv:1301.6201 [math]},
	author = {Fong, Brendan},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.6201},
	keywords = {Mathematics - Probability},
	annote = {Comment: 72 pages}
}


@article{ranganath_multiple_2018,
	title = {Multiple {Causal} {Inference} with {Latent} {Confounding}},
	url = {http://arxiv.org/abs/1805.08273},
	abstract = {Causal inference from observational data requires assumptions. These assumptions range from measuring confounders to identifying instruments. Traditionally, these assumptions have focused on estimation in a single causal problem. In this work, we develop techniques for causal estimation in causal problems with multiple treatments. We develop two assumptions based on shared confounding between treatments and independence of treatments given the confounder. Together these assumptions lead to a confounder estimator regularized by mutual information. For this estimator, we develop a tractable lower bound. To fit the outcome model, we use the residual information in the treatments given the confounder. We validate on simulations and an example from clinical medicine.},
	urldate = {2018-10-18},
	journal = {arXiv:1805.08273 [cs, stat]},
	author = {Ranganath, Rajesh and Perotte, Adler},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08273},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1805.08273 PDF:/home/users/u4533535/Zotero/storage/36YXHYZC/Ranganath and Perotte - 2018 - Multiple Causal Inference with Latent Confounding.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/ZF7FV6YS/1805.html:text/html}
}


@book{hernan_causal_2018,
	title = {Causal {Inference}},
	language = {en-us},
	publisher = {Chapman \& Hall/CRC},
	author = {Hernán, MA and Robins, JM},
	year = {2018},
	file = {Snapshot:/home/users/u4533535/Zotero/storage/3PD2MVZA/causal-inference-book.html:text/html}
}



@incollection{weirich_causal_2016,
	edition = {Winter 2016},
	title = {Causal {Decision} {Theory}},
	url = {https://plato.stanford.edu/archives/win2016/entries/decision-causal/},
	abstract = {Causal decision theory adopts principles of rational choice thatattend to an act’s consequences. It maintains that an account ofrational choice must use causality to identify the considerations thatmake a choice rational., Given a set of options constituting a decision problem, decisiontheory recommends an option that maximizes utility, that is, an optionwhose utility equals or exceeds the utility of every other option. Itevaluates an option’s utility by calculating the option’sexpected utility. It uses probabilities and utilities of anoption’s possible outcomes to define an option’s expectedutility. The probabilities depend on the option. Causal decisiontheory takes the dependence to be causal rather than merelyevidential., This essay explains causal decision theory, reviews its history,describes current research in causal decision theory, and surveys thetheory’s philosophical foundations. The literature on causaldecision theory is vast, and this essay covers only a portion ofit.},
	urldate = {2018-11-15},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Weirich, Paul},
	editor = {Zalta, Edward N.},
	year = {2016},
	file = {SEP - Snapshot:/home/users/u4533535/Zotero/storage/VFCIH54I/decision-causal.html:text/html}
}


@article{peters_structural_2013,
	title = {Structural {Intervention} {Distance} ({SID}) for {Evaluating} {Causal} {Graphs}},
	url = {http://arxiv.org/abs/1306.1043},
	abstract = {Causal inference relies on the structure of a graph, often a directed acyclic graph (DAG). Different graphs may result in different causal inference statements and different intervention distributions. To quantify such differences, we propose a (pre-) distance between DAGs, the structural intervention distance (SID). The SID is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements. It is therefore well-suited for evaluating graphs that are used for computing interventions. Instead of DAGs it is also possible to compare CPDAGs, completed partially directed acyclic graphs that represent Markov equivalence classes. Since it differs significantly from the popular Structural Hamming Distance (SHD), the SID constitutes a valuable additional measure. We discuss properties of this distance and provide an efficient implementation with software code available on the first author's homepage (an R package is under construction).},
	urldate = {2018-02-20},
	journal = {arXiv:1306.1043 [stat]},
	author = {Peters, Jonas and Bühlmann, Peter},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.1043},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1306.1043 PDF:/home/users/u4533535/Zotero/storage/XKLD3IZ9/Peters and Bühlmann - 2013 - Structural Intervention Distance (SID) for Evaluat.pdf:application/pdf;arXiv.org Snapshot:/home/users/u4533535/Zotero/storage/BFJYMINB/1306.html:text/html}
}


@article{jeffrey_logic_1981,
	title = {The logic of decision defended},
	volume = {48},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/BF01063989},
	doi = {10.1007/BF01063989},
	abstract = {The approach to decision theory floated in my 1965 book is reviewed (I), challenged in various related ways (II–V) and defended, firstad hoc (II–IV) and then by a general argument of Ellery Ells's (VI). Finally, causal decision theory (in a version sketched in VII) is exhibited as a special case of my 1965 theory, according to the Eellsian argument.},
	language = {en},
	number = {3},
	urldate = {2018-11-19},
	journal = {Synthese},
	author = {Jeffrey, Richard},
	month = sep,
	year = {1981},
	keywords = {Causal Decision, Causal Decision Theory, Decision Theory, General Argument},
	pages = {473--492},
	file = {Springer Full Text PDF:/home/users/u4533535/Zotero/storage/ZXMJH69T/Jeffrey - 1981 - The logic of decision defended.pdf:application/pdf}
}


@inproceedings{dawid_beware_2010,
	title = {Beware of the {DAG}!},
	url = {http://proceedings.mlr.press/v6/dawid10a.html},
	abstract = {Directed acyclic graph (DAG) models are popular tools for describing causal relationships and for guiding attempts to learn them from data.  They appear to supply a means of extracting causal concl...},
	language = {en},
	urldate = {2018-03-09},
	booktitle = {Causality: {Objectives} and {Assessment}},
	author = {Dawid, A. Philip},
	month = feb,
	year = {2010},
	pages = {59--86},
	file = {Full Text PDF:C\:\\Users\\david\\Zotero\\storage\\K2FLIM3E\\Dawid - 2010 - Beware of the DAG!.pdf:application/pdf;Snapshot:C\:\\Users\\david\\Zotero\\storage\\3BBKAGH9\\dawid10a.html:text/html}
}


@article{wu_toward_2018,
	title = {Toward an {AI} {Physicist} for {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1810.10525},
	abstract = {We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's Razor, unification, and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and "snap" learned theories into simple symbolic formulas. Theories are stored in a "theory hub", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the "AI Physicist" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.},
	urldate = {2018-11-27},
	journal = {arXiv:1810.10525 [cond-mat, physics:physics]},
	author = {Wu, Tailin and Tegmark, Max},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.10525},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics},
	annote = {Comment: Typos fixed, references added, discussion improved. 18 pages, 7 figs}
}


@article{gupta_intention--treat_2011,
	title = {Intention-to-treat concept: {A} review},
	volume = {2},
	issn = {2229-3485},
	shorttitle = {Intention-to-treat concept},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/},
	doi = {10.4103/2229-3485.83221},
	abstract = {Randomized controlled trials often suffer from two major complications, i.e., noncompliance and missing outcomes. One potential solution to this problem is a statistical concept called intention-to-treat (ITT) analysis. ITT analysis includes every subject who is randomized according to randomized treatment assignment. It ignores noncompliance, protocol deviations, withdrawal, and anything that happens after randomization. ITT analysis maintains prognostic balance generated from the original random treatment allocation. In ITT analysis, estimate of treatment effect is generally conservative. A better application of the ITT approach is possible if complete outcome data are available for all randomized subjects. Per-protocol population is defined as a subset of the ITT population who completed the study without any major protocol violations.},
	number = {3},
	urldate = {2018-12-06},
	journal = {Perspectives in Clinical Research},
	author = {Gupta, Sandeep K.},
	year = {2011},
	pmid = {21897887},
	pmcid = {PMC3159210},
	pages = {109--112}
}